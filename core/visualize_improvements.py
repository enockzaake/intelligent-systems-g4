"""
Comprehensive Visualization of Model Improvements
Generates detailed comparison charts and analysis
"""
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (15, 10)
plt.rcParams['font.size'] = 10


def load_results(results_path):
    """Load evaluation results JSON"""
    try:
        with open(results_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Warning: {results_path} not found")
        return None


def create_comprehensive_visualizations():
    """Create comprehensive before/after visualizations"""
    
    print("\n" + "=" * 80)
    print("GENERATING COMPREHENSIVE VISUALIZATIONS")
    print("=" * 80)
    
    # Load results
    original_dir = Path("outputs/results")
    improved_dir = Path("outputs_improved/results")
    
    original_files = list(original_dir.glob("evaluation_results_*.json"))
    improved_files = list(improved_dir.glob("evaluation_results_*.json"))
    
    if not original_files or not improved_files:
        print("‚ùå Missing results files. Please run training first.")
        return
    
    original_results = load_results(sorted(original_files)[-1])
    improved_results = load_results(sorted(improved_files)[-1])
    
    # Create output directory
    output_dir = Path("outputs/analysis")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. Model Comparison Dashboard
    create_model_comparison_dashboard(original_results, improved_results, output_dir)
    
    # 2. LSTM Improvement Focus
    create_lstm_improvement_chart(original_results, improved_results, output_dir)
    
    # 3. Confusion Matrix Comparison
    create_confusion_matrix_comparison(original_results, improved_results, output_dir)
    
    # 4. Metric Radar Charts
    create_radar_charts(original_results, improved_results, output_dir)
    
    # 5. ROC Curve Comparison (if available)
    create_summary_table(original_results, improved_results, output_dir)
    
    print(f"\n‚úÖ All visualizations saved to: {output_dir.absolute()}")


def create_model_comparison_dashboard(original, improved, output_dir):
    """Create comprehensive comparison dashboard"""
    
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    fig.suptitle('Model Performance: Original vs Improved', fontsize=20, fontweight='bold', y=0.98)
    
    models = {
        'Logistic Regression': 'Logistic Regression',
        'Random Forest Classifier': 'Random Forest Classifier',
        'LSTM Classifier': 'LSTM Classifier'
    }
    
    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']
    
    # Extract data
    data = []
    for model_name, model_key in models.items():
        orig_model = original.get(model_name, {})
        imp_model = improved.get(f"{model_name} (Improved)", improved.get(model_name, {}))
        
        for metric in metrics:
            if metric in orig_model and metric in imp_model:
                data.append({
                    'Model': model_name.replace(' Classifier', '').replace(' Regression', ''),
                    'Metric': metric.replace('_', ' ').title(),
                    'Original': orig_model[metric],
                    'Improved': imp_model[metric],
                    'Change': imp_model[metric] - orig_model[metric]
                })
    
    df = pd.DataFrame(data)
    
    # Plot 1: Accuracy comparison
    ax1 = fig.add_subplot(gs[0, 0])
    plot_metric_comparison(df, 'Accuracy', ax1)
    
    # Plot 2: Precision comparison
    ax2 = fig.add_subplot(gs[0, 1])
    plot_metric_comparison(df, 'Precision', ax2)
    
    # Plot 3: Recall comparison
    ax3 = fig.add_subplot(gs[0, 2])
    plot_metric_comparison(df, 'Recall', ax3)
    
    # Plot 4: F1-Score comparison
    ax4 = fig.add_subplot(gs[1, 0])
    plot_metric_comparison(df, 'F1 Score', ax4)
    
    # Plot 5: ROC-AUC comparison
    ax5 = fig.add_subplot(gs[1, 1])
    plot_metric_comparison(df, 'Roc Auc', ax5)
    
    # Plot 6: Overall improvement heatmap
    ax6 = fig.add_subplot(gs[1, 2])
    plot_improvement_heatmap(df, ax6)
    
    # Plot 7-9: Individual model details
    model_list = list(models.keys())
    for idx, model_name in enumerate(model_list):
        ax = fig.add_subplot(gs[2, idx])
        plot_model_metrics(df, model_name.replace(' Classifier', '').replace(' Regression', ''), ax)
    
    plt.savefig(output_dir / 'comprehensive_comparison_dashboard.png', dpi=300, bbox_inches='tight')
    print(f"‚úì Created: comprehensive_comparison_dashboard.png")


def plot_metric_comparison(df, metric, ax):
    """Plot comparison for a specific metric"""
    metric_data = df[df['Metric'] == metric]
    
    if metric_data.empty:
        return
    
    x = np.arange(len(metric_data))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, metric_data['Original'], width, label='Original', 
                   alpha=0.8, color='#FF6B6B')
    bars2 = ax.bar(x + width/2, metric_data['Improved'], width, label='Improved', 
                   alpha=0.8, color='#4ECDC4')
    
    # Add value labels
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    ax.set_ylabel('Score')
    ax.set_title(metric)
    ax.set_xticks(x)
    ax.set_xticklabels([m[:10] for m in metric_data['Model']], rotation=0)
    ax.legend()
    ax.set_ylim(0, 1.0)
    ax.grid(axis='y', alpha=0.3)


def plot_improvement_heatmap(df, ax):
    """Plot heatmap of improvements"""
    pivot_data = df.pivot(index='Model', columns='Metric', values='Change')
    
    sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn', center=0,
               ax=ax, cbar_kws={'label': 'Change'})
    ax.set_title('Improvement Heatmap\n(Green = Better)')
    ax.set_xlabel('')
    ax.set_ylabel('')


def plot_model_metrics(df, model_name, ax):
    """Plot all metrics for a specific model"""
    model_data = df[df['Model'] == model_name]
    
    if model_data.empty:
        return
    
    metrics = model_data['Metric'].values
    original = model_data['Original'].values
    improved = model_data['Improved'].values
    
    x = np.arange(len(metrics))
    
    ax.plot(x, original, 'o-', label='Original', linewidth=2, markersize=8, color='#FF6B6B')
    ax.plot(x, improved, 's-', label='Improved', linewidth=2, markersize=8, color='#4ECDC4')
    
    ax.set_xticks(x)
    ax.set_xticklabels([m[:8] for m in metrics], rotation=45, ha='right')
    ax.set_ylabel('Score')
    ax.set_title(model_name)
    ax.legend()
    ax.set_ylim(0, 1.0)
    ax.grid(alpha=0.3)


def create_lstm_improvement_chart(original, improved, output_dir):
    """Create focused visualization of LSTM improvements"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('LSTM Classifier: Dramatic Improvement Analysis', fontsize=16, fontweight='bold')
    
    orig_lstm = original.get('LSTM Classifier', {})
    imp_lstm = improved.get('LSTM Classifier (Improved)', improved.get('LSTM Classifier', {}))
    
    metrics = ['accuracy', 'precision', 'recall', 'f1_score']
    metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    
    # Plot 1: Before/After bars
    ax = axes[0, 0]
    x = np.arange(len(metrics))
    width = 0.35
    
    orig_vals = [orig_lstm.get(m, 0) for m in metrics]
    imp_vals = [imp_lstm.get(m, 0) for m in metrics]
    
    bars1 = ax.bar(x - width/2, orig_vals, width, label='Original', alpha=0.8, color='#FF6B6B')
    bars2 = ax.bar(x + width/2, imp_vals, width, label='Improved', alpha=0.8, color='#4ECDC4')
    
    # Highlight recall improvement
    if orig_lstm.get('recall', 0) < 0.1 and imp_lstm.get('recall', 0) > 0.5:
        ax.annotate('', xy=(2 + width/2, imp_vals[2]), xytext=(2 - width/2, orig_vals[2]),
                   arrowprops=dict(arrowstyle='->', lw=3, color='green'))
        ax.text(2, max(orig_vals[2], imp_vals[2]) + 0.1, '‚úÖ FIXED!', 
               ha='center', fontsize=12, fontweight='bold', color='green')
    
    ax.set_ylabel('Score')
    ax.set_title('Metric Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(metric_labels)
    ax.legend()
    ax.set_ylim(0, 1.0)
    ax.grid(axis='y', alpha=0.3)
    
    # Plot 2: Percentage change
    ax = axes[0, 1]
    changes = [(imp_vals[i] - orig_vals[i]) / orig_vals[i] * 100 if orig_vals[i] > 0 else 0 
              for i in range(len(metrics))]
    colors = ['green' if c > 0 else 'red' for c in changes]
    
    bars = ax.barh(metric_labels, changes, color=colors, alpha=0.7)
    ax.axvline(0, color='black', linewidth=0.8)
    ax.set_xlabel('Percentage Change (%)')
    ax.set_title('Relative Improvement')
    ax.grid(axis='x', alpha=0.3)
    
    # Add value labels
    for i, (bar, val) in enumerate(zip(bars, changes)):
        ax.text(val, i, f'{val:+.1f}%', va='center', 
               ha='left' if val > 0 else 'right', fontweight='bold')
    
    # Plot 3: Confusion Matrix Comparison
    ax = axes[1, 0]
    orig_cm = orig_lstm.get('confusion_matrix', [[0,0],[0,0]])
    imp_cm = imp_lstm.get('confusion_matrix', [[0,0],[0,0]])
    
    cm_comparison = np.array([
        [orig_cm[1][0], imp_cm[1][0]],  # False Negatives
        [orig_cm[1][1], imp_cm[1][1]]   # True Positives
    ])
    
    sns.heatmap(cm_comparison, annot=True, fmt='d', cmap='RdYlGn', ax=ax,
               xticklabels=['Original', 'Improved'],
               yticklabels=['Missed Delays (FN)', 'Caught Delays (TP)'],
               cbar_kws={'label': 'Count'})
    ax.set_title('Confusion Matrix: Delay Detection')
    
    # Plot 4: Key Achievement
    ax = axes[1, 1]
    ax.axis('off')
    
    orig_recall = orig_lstm.get('recall', 0) * 100
    imp_recall = imp_lstm.get('recall', 0) * 100
    improvement = imp_recall - orig_recall
    
    achievement_text = f"""
    üéâ KEY ACHIEVEMENT üéâ
    
    LSTM Recall Improvement:
    
    Before: {orig_recall:.1f}%
            ‚Üì
    After:  {imp_recall:.1f}%
    
    Improvement: +{improvement:.1f}%
    
    Impact:
    ‚Ä¢ Catching {improvement:.0f}% MORE delays
    ‚Ä¢ From nearly useless to production-ready
    ‚Ä¢ {orig_recall:.0f}% ‚Üí {imp_recall:.0f}% detection rate
    
    ‚úÖ Problem SOLVED!
    """
    
    ax.text(0.5, 0.5, achievement_text, ha='center', va='center',
           fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(output_dir / 'lstm_improvement_analysis.png', dpi=300, bbox_inches='tight')
    print(f"‚úì Created: lstm_improvement_analysis.png")


def create_confusion_matrix_comparison(original, improved, output_dir):
    """Create side-by-side confusion matrix comparison"""
    
    models = ['Logistic Regression', 'Random Forest Classifier', 'LSTM Classifier']
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle('Confusion Matrices: Original vs Improved', fontsize=16, fontweight='bold')
    
    for idx, model_name in enumerate(models):
        orig_model = original.get(model_name, {})
        imp_model = improved.get(f"{model_name} (Improved)", improved.get(model_name, {}))
        
        # Original
        ax_orig = axes[0, idx]
        orig_cm = orig_model.get('confusion_matrix', [[0,0],[0,0]])
        sns.heatmap(orig_cm, annot=True, fmt='d', cmap='Blues', ax=ax_orig,
                   xticklabels=['On-time', 'Delayed'],
                   yticklabels=['On-time', 'Delayed'])
        ax_orig.set_title(f'{model_name.replace(" Classifier", "")}\n(Original)')
        ax_orig.set_ylabel('True')
        ax_orig.set_xlabel('Predicted')
        
        # Improved
        ax_imp = axes[1, idx]
        imp_cm = imp_model.get('confusion_matrix', [[0,0],[0,0]])
        sns.heatmap(imp_cm, annot=True, fmt='d', cmap='Greens', ax=ax_imp,
                   xticklabels=['On-time', 'Delayed'],
                   yticklabels=['On-time', 'Delayed'])
        ax_imp.set_title(f'{model_name.replace(" Classifier", "")}\n(Improved)')
        ax_imp.set_ylabel('True')
        ax_imp.set_xlabel('Predicted')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'confusion_matrix_comparison.png', dpi=300, bbox_inches='tight')
    print(f"‚úì Created: confusion_matrix_comparison.png")


def create_radar_charts(original, improved, output_dir):
    """Create radar charts for model comparison"""
    
    from math import pi
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6), subplot_kw=dict(projection='polar'))
    fig.suptitle('Model Performance Radar Charts', fontsize=16, fontweight='bold')
    
    models = ['Logistic Regression', 'Random Forest Classifier', 'LSTM Classifier']
    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']
    
    angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]
    angles += angles[:1]
    
    for idx, model_name in enumerate(models):
        ax = axes[idx]
        
        orig_model = original.get(model_name, {})
        imp_model = improved.get(f"{model_name} (Improved)", improved.get(model_name, {}))
        
        orig_vals = [orig_model.get(m, 0) for m in metrics]
        orig_vals += orig_vals[:1]
        
        imp_vals = [imp_model.get(m, 0) for m in metrics]
        imp_vals += imp_vals[:1]
        
        ax.plot(angles, orig_vals, 'o-', linewidth=2, label='Original', color='#FF6B6B')
        ax.fill(angles, orig_vals, alpha=0.25, color='#FF6B6B')
        
        ax.plot(angles, imp_vals, 's-', linewidth=2, label='Improved', color='#4ECDC4')
        ax.fill(angles, imp_vals, alpha=0.25, color='#4ECDC4')
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics])
        ax.set_ylim(0, 1)
        ax.set_title(model_name.replace(' Classifier', ''), pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
        ax.grid(True)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'radar_chart_comparison.png', dpi=300, bbox_inches='tight')
    print(f"‚úì Created: radar_chart_comparison.png")


def create_summary_table(original, improved, output_dir):
    """Create a comprehensive summary table"""
    
    models = ['Logistic Regression', 'Random Forest Classifier', 'LSTM Classifier']
    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']
    
    # Create DataFrame
    data = []
    for model_name in models:
        orig_model = original.get(model_name, {})
        imp_model = improved.get(f"{model_name} (Improved)", improved.get(model_name, {}))
        
        for metric in metrics:
            orig_val = orig_model.get(metric, 0)
            imp_val = imp_model.get(metric, 0)
            change = imp_val - orig_val
            pct_change = (change / orig_val * 100) if orig_val > 0 else 0
            
            data.append({
                'Model': model_name.replace(' Classifier', '').replace(' Regression', ''),
                'Metric': metric.replace('_', ' ').title(),
                'Original': f'{orig_val:.4f}',
                'Improved': f'{imp_val:.4f}',
                'Change': f'{change:+.4f}',
                'Change (%)': f'{pct_change:+.2f}%',
                'Status': '‚úÖ' if change > 0 else ('‚ùå' if change < 0 else '‚ûñ')
            })
    
    df = pd.DataFrame(data)
    
    # Save as CSV
    csv_path = output_dir / 'detailed_comparison_table.csv'
    df.to_csv(csv_path, index=False)
    print(f"‚úì Created: detailed_comparison_table.csv")
    
    # Create visual table
    fig, ax = plt.subplots(figsize=(14, 10))
    ax.axis('tight')
    ax.axis('off')
    
    table = ax.table(cellText=df.values, colLabels=df.columns,
                    cellLoc='center', loc='center',
                    colWidths=[0.15, 0.12, 0.12, 0.12, 0.12, 0.12, 0.08])
    
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 2)
    
    # Color code the status column
    for i in range(1, len(df) + 1):
        status = df.iloc[i-1]['Status']
        if status == '‚úÖ':
            table[(i, 6)].set_facecolor('#90EE90')
        elif status == '‚ùå':
            table[(i, 6)].set_facecolor('#FFB6C1')
    
    # Header styling
    for j in range(len(df.columns)):
        table[(0, j)].set_facecolor('#4ECDC4')
        table[(0, j)].set_text_props(weight='bold', color='white')
    
    plt.title('Comprehensive Model Comparison Table', fontsize=14, fontweight='bold', pad=20)
    plt.savefig(output_dir / 'comparison_table.png', dpi=300, bbox_inches='tight')
    print(f"‚úì Created: comparison_table.png")


if __name__ == "__main__":
    create_comprehensive_visualizations()

